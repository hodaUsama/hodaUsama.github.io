---
layout: post
title: "ğŸ” From Limits to Smoothness: Transformations, Limits, Continuity & Differentiability"
date: 2025-06-17
categories: [calculus, beginner]
tags: [functions, limits, continuity, differentiability, transformations, visualization, python, math]
description: "Understand functions as geometric transformations, visualize limits, and grasp continuity and differentiability with Python and math-based insights."
math: true
---

# ğŸ” Functions as Transformations

A **function** can be seen as a machine: it takes input values and returns output values. But in a more visual and geometric sense, a function can be thought of as a **transformation** â€” changing or warping the input space into a new shape.

---

<div class="series-nav">
  <p>ğŸ“š <strong>This post is part of the "Intro to Calculus" series</strong></p> 
   <p>ğŸ”™ <strong>Previously:</strong> <a href="/posts/visualizing-multivariable-functions/">Visualizing Multivariable Functions: Contour Plots, Vector-Valued Functions & Vector Fields (Beginner's Guide)</a></p>
   <p>ğŸ”œ <strong>Next:</strong> <a href="/posts/what-is-dervative/" style="color:#1E90FF;">What is a Derivative? (Beginnerâ€™s Guide to Calculus for ML)</a></p>
</div>

---

### ğŸ“š Mathematical Idea

If we start with a point \\( x \in \mathbb{R} \\), a function \\( f(x) \\) maps it to another real number \\( y \\).  
For a 2D transformation:

\\[
f(x, y) = (x, y^2)
\\]

This "bends" the coordinate plane vertically â€” keeping \\( x \\) unchanged but squaring the \\( y \\)-coordinate.

### ğŸ§ª Python Example: Transforming a Grid

_Visual: Show a grid before and after the transformation \\( f(x, y) = (x, y^2) \\)_

```python
import numpy as np
import matplotlib.pyplot as plt

x, y = np.meshgrid(np.linspace(-2, 2, 20), np.linspace(-2, 2, 20))
x_new = x
y_new = y ** 2

fig, axs = plt.subplots(1, 2, figsize=(10, 4))
axs[0].quiver(x, y, np.zeros_like(x), np.zeros_like(y), color='gray')
axs[0].set_title("Original Grid")
axs[0].axis('equal')

axs[1].quiver(x_new, y_new, np.zeros_like(x), np.zeros_like(y), color='teal')
axs[1].set_title("Transformed Grid: $f(x, y) = (x, y^2)$")
axs[1].axis('equal')

plt.tight_layout()
plt.show()
```
---

<figure style="text-align: center;">
  <img src="/assets/images/transformation_grid.png" alt="Left panel shows the original 2D grid.
Right panel shows how the transformation bends the grid vertically, curving the 
ğ‘¦ y-coordinates upward while preserving the  ğ‘¥ x-axis." width="350" />
  <figcaption>
    <em>
    Left panel shows the original 2D grid.
Right panel shows how the transformation bends the grid vertically, curving the 
ğ‘¦ y-coordinates upward while preserving the  ğ‘¥ x-axis.
    </em>
  </figcaption>
</figure>
---

# ğŸ“‰ What Is a Limit?

A **limit** describes what a function is approaching as the input gets closer and closer to a specific value â€” even if the function is not *defined* at that value.

### ğŸ“š Mathematical Definition

\\[
\lim_{x \to a} f(x) = L
\\]

This means: as \\( x \\) gets arbitrarily close to \\( a \\), the value of \\( f(x) \\) gets arbitrarily close to \\( L \\).

### ğŸ” Example:

Let:

\\[
f(x) = \frac{x^2 - 1}{x - 1}
\\]

At \\( x = 1 \\), the function is undefined â€” but we can factor:

\\[
f(x) = \frac{(x - 1)(x + 1)}{x - 1} = x + 1 \quad \text{(for } x \neq 1 \text{)}
\\]

So:

\\[
\lim_{x \to 1} f(x) = 2
\\]

### ğŸ§ª Python Visualization

```python
x = np.linspace(0.5, 1.5, 200)
y = (x**2 - 1)/(x - 1)

plt.plot(x, y, label=r'$f(x) = \frac{x^2 - 1}{x - 1}$')
plt.axvline(1, color='red', linestyle='--', label='x = 1')
plt.axhline(2, color='green', linestyle='--', label='Limit = 2')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Limit Approaching x = 1')
plt.legend()
plt.grid(True)
plt.show()
```

---
![Plot showing the limit of \( f(x) = \frac{x^2 - 1}{x - 1} \) as \( x \to 1 \)](/assets/images/limit_at_x_1.png)

---
# ğŸ§© Examples of Limits in Functions

### ğŸ”¹ One-Sided Limits

\\[
\lim_{x \to a^-} f(x) \quad \text{and} \quad \lim_{x \to a^+} f(x)
\\]

The full limit exists only when both sides agree.

### ğŸ”¹ Piecewise Example:

\\[
f(x) = \begin{cases} x^2 & \text{if } x < 1 \\ 3 & \text{if } x = 1 \\ 2 - x & \text{if } x > 1 \end{cases}
\\]

Then:

\\[
\lim_{x \to 1^-} f(x) = 1 \quad \text{and} \quad \lim_{x \to 1^+} f(x) = 1 \Rightarrow \lim_{x \to 1} f(x) = 1
\\]

But:

\\[
f(1) = 3 \neq \lim_{x \to 1} f(x)
\\]

So the function has a removable discontinuity at \\( x = 1 \\).

---
![Piecewise function with removable discontinuity at \( x = 1 \)](/assets/images/piecewise_discontinuity.png)
---
# ğŸ”— Continuity and Differentiability

### âœ… Continuity

A function \\( f(x) \\) is **continuous** at \\( x = a \\) if:

1. \\( f(a) \\) is defined  
2. \\( \lim_{x \to a} f(x) \\) exists  
3. \\( \lim_{x \to a} f(x) = f(a) \\)

### âœ… Differentiability

A function is **differentiable** at \\( x = a \\) if it is continuous and smooth (no sharp corners or cusps).

### ğŸ” Example: \\( f(x) = |x| \\)

- Continuous everywhere  
- **Not differentiable** at \\( x = 0 \\) because of a sharp point

### ğŸ§ª Python Visualization

```python
x = np.linspace(-2, 2, 400)
y = np.abs(x)

plt.plot(x, y, label=r'$f(x) = |x|$', color='blue')
plt.axvline(0, color='red', linestyle='--', label='x = 0')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Absolute Value: Continuous but Not Differentiable at x = 0')
plt.legend()
plt.grid(True)
plt.show()
```
---
![Absolute value function is continuous but not differentiable at \( x = 0 \)](/assets/images/abs_value_not_differentiable.png)

---

## ğŸ¤– Relevance to Machine Learning

Understanding **limits**, **continuity**, and **differentiability** is essential for many foundational ideas in machine learning and deep learning:

- **ğŸ§  Gradient Descent & Optimization**  
  Most learning algorithms (like gradient descent) rely on functions being continuous and differentiable so we can compute smooth gradients to minimize loss.

- **ğŸ” Backpropagation**  
  Neural networks use the chain rule to propagate error gradients backward â€” which requires differentiable activation functions and loss functions.

- **ğŸ“‰ Loss Surfaces**  
  The cost or loss function must be smooth and continuous for optimizers to navigate toward minima efficiently. Sharp discontinuities can trap or mislead optimization.

- **ğŸ§© Activation Functions**  
  Common activations (ReLU, sigmoid, tanh) are chosen based on their continuity and differentiability â€” affecting both model capacity and training dynamics.

- **ğŸ“ Regularization & Generalization**  
  Techniques like L2 regularization implicitly promote smoother (more continuous and differentiable) functions, which helps with generalization and avoiding overfitting.

- **âš ï¸ Adversarial Robustness**  
  Discontinuous or non-differentiable spots in the model behavior can be exploited by adversarial examples. Smoothness leads to more stable and robust models.

---

> ğŸ§­ **Key Insight**: If your model isnâ€™t differentiable, gradient-based learning breaks down. Smoothness isn't just elegant â€” itâ€™s essential!


---

<details class="level-up-box">
<summary class="level-up-title">ğŸ§  Level Up</summary>
<div class="level-up-content">
<ul>
  <li>ğŸ”„ <b>Functions as Mappings:</b> Think of every function as a way to reshape input space â€” this is crucial for understanding transformations in deep learning layers.</li>
  <li>ğŸ“ <b>Limits and Precision:</b> Mastering limits builds your intuition for numerical stability, convergence, and approximation in ML algorithms.</li>
  <li>ğŸ“ <b>Continuity in Practice:</b> Continuous loss functions ensure smooth training. Discontinuities can cause sudden optimization failures.</li>
  <li>ğŸ§® <b>Differentiability = Learnability:</b> If a function isnâ€™t differentiable, gradient-based methods (like backpropagation) wonâ€™t work.</li>
  <li>ğŸ“‰ <b>Piecewise Behavior:</b> Recognize when piecewise models like ReLU introduce non-differentiable points â€” and how this affects learning speed.</li>
  <li>ğŸ§© <b>Function Smoothness:</b> Smooth, continuous, and differentiable models generalize better and are more robust to noisy data.</li>
</ul>
</div>
</details>

---
<details class="custom-box custom-best">
<summary><strong>âœ… Best Practices</strong></summary>

<ul>
  <li>ğŸ“Œ <b>Clearly define your domain</b>: Before analyzing limits or continuity, specify where the function is defined and what happens near edges.</li>
  <li>ğŸ” <b>Check one-sided limits</b>: Always test left-hand and right-hand limits â€” especially for piecewise or discontinuous functions.</li>
  <li>ğŸ“‰ <b>Use simple plots for intuition</b>: Visualizing limits or corners (like in \(|x|\)) makes differentiability easier to grasp.</li>
  <li>ğŸ§® <b>Simplify before evaluating</b>: Use algebra (factoring, cancelling) to rewrite functions when limits seem undefined.</li>
  <li>ğŸ§  <b>Distinguish continuity from differentiability</b>: Remember, a function can be continuous but not differentiable.</li>
  <li>ğŸ’¡ <b>Test critical points</b>: Especially around \(x = 0\), corners, or undefined values â€” those are the hotspots for discontinuity or non-smooth behavior.</li>
</ul>
</details>

---

<details class="custom-box custom-warning">
<summary><strong>âš ï¸ Common Pitfalls</strong></summary>

<ul>
  <li>âŒ <b>Assuming all functions are smooth</b>: Not all continuous functions are differentiable. Donâ€™t confuse them.</li>
  <li>âŒ <b>Forgetting removable discontinuities</b>: A function might have a limit even when itâ€™s undefined at a point.</li>
  <li>âŒ <b>Using only numeric evaluation</b>: Relying only on plotting or calculators can miss underlying structure â€” combine with algebra.</li>
  <li>âŒ <b>Overlooking piecewise definitions</b>: For functions defined in parts, always check each region separately.</li>
  <li>âŒ <b>Ignoring symmetry</b>: Functions like even/odd functions or absolute values have special properties that affect continuity and smoothness.</li>
</ul>
</details>

---
## ğŸ“Œ <span style="color:#9370DB;">Try It Yourself</span>

<details>
<summary>ğŸ“‰ <strong>Limit Challenge:</strong> What is  
\(\displaystyle \lim_{x \to 2} \frac{x^2 - 4}{x - 2} \) ?</summary>

ğŸ§  Step-by-step:<br>
- Factor numerator: \( x^2 - 4 = (x - 2)(x + 2) \)<br>
- Cancel terms: \( \frac{(x - 2)(x + 2)}{x - 2} = x + 2 \) (for \( x \ne 2 \))<br>

âœ… Final Answer:
\[
\lim_{x \to 2} \frac{x^2 - 4}{x - 2} = 4
\]
</details>

---

<details>
<summary>ğŸ“ˆ <strong>Continuity Check:</strong> Is the function  
\[
f(x) = \begin{cases}
x^2 & x < 1 \\\\
3 & x = 1 \\\\
2 - x & x > 1
\end{cases}
\]
continuous at \( x = 1 \) ?</summary>

ğŸ§  Step-by-step:<br>
- Left-hand limit: \( \lim_{x \to 1^-} f(x) = 1^2 = 1 \)<br>
- Right-hand limit: \( \lim_{x \to 1^+} f(x) = 2 - 1 = 1 \)<br>
- But \( f(1) = 3 \) ğŸ¤”<br>

âŒ Not continuous!  
âœ… Final Answer:
\[
\lim_{x \to 1} f(x) = 1 \ne f(1)
\]
</details>

---

<details>
<summary>ğŸ“ <strong>Differentiability Test:</strong> Is  
\( f(x) = |x| \) differentiable at \( x = 0 \)?</summary>

ğŸ§  Hint:<br>
- Left-hand derivative: \( f'(x) = -1 \)<br>
- Right-hand derivative: \( f'(x) = 1 \)<br>

âŒ Derivatives don't match at \( x = 0 \), so not differentiable!

âœ… Final Answer:
\[
f(x) = |x| \text{ is not differentiable at } x = 0
\]
</details>

---

<details>
<summary>ğŸŒ <strong>Transformation Intuition:</strong> What does  
\( f(x, y) = (x, y^2) \) do to the plane?</summary>

ğŸ§  Insight:<br>
- Keeps \( x \) the same  
- Squashes negative \( y \) to positive  
- Bends the grid into a parabolic shape

âœ… Visualization:
- Try plotting the grid with original vs transformed coordinates!
</details>

---

## âœ… <span style="color:#228B22;">Summary</span>

Letâ€™s wrap up the key ideas from this post:

---

<table>
  <thead>
    <tr>
      <th><span style="color:#1E90FF;">Topic</span></th>
      <th><span style="color:#2E8B57;">Summary</span></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Function as Transformation</td>
      <td>Warping or reshaping input space â€” e.g., <span style="color:#DA70D6;">\( f(x, y) = (x, y^2) \)</span> folds the plane</td>
    </tr>
    <tr>
      <td>Limits</td>
      <td>Describe how a function behaves near a point â€” not just at it</td>
    </tr>
    <tr>
      <td>Continuity</td>
      <td>Function is continuous if limit exists and matches the value at the point</td>
    </tr>
    <tr>
      <td>Differentiability</td>
      <td>Smoothness â€” function must have a well-defined slope (no corners)</td>
    </tr>
    <tr>
      <td>Relevance to ML</td>
      <td>Essential for <b>gradients</b>, <b>backpropagation</b>, and <span style="color:#FF6347;">smooth training</span></td>
    </tr>
  </tbody>
</table>
---

## ğŸ’¬ <span style="color:#4B0082;">Got a question or suggestion?</span>

Leave a comment below â€” Iâ€™d love to hear your thoughts or help if something was unclear.

---
## ğŸ§­ <span style="color:#9370DB;">Next Up</span>

Now that youâ€™ve explored how functions behave through transformations, limits, and smoothness, itâ€™s time to zoom in on how they **change** â€” and how we measure that change precisely.

In the upcoming post, weâ€™ll dive into:

- What a **gradient** really is â€” and how it generalizes the derivative to higher dimensions  
- The meaning of **instantaneous rate of change** in both math and machine learning  
- How **limits** give rise to **derivatives**, step by step  
- Using **gradients for approximation** and direction-finding in complex systems  
- How to **calculate derivatives** symbolically and numerically using Python  

ğŸ§  These tools are essential for optimization, learning, and understanding the terrain of functions.

Stay tuned â€” weâ€™re about to unlock the core mechanics of calculus and machine learning!
